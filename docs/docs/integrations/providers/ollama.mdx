# Ollama

>[Ollama](https://ollama.com/) allows you to run open-source large language models, 
> such as [Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/), locally.
>
>`Ollama` bundles model weights, configuration, and data into a single package, defined by a Modelfile. 
>It optimizes setup and configuration details, including GPU usage.
>For a complete list of supported models and model variants, see the [Ollama model library](https://ollama.ai/library).

See [this guide](/docs/how_to/local_llms) for more details
on how to use `Ollama` with LangChain.

## Installation and Setup
Follow [these instructions](https://github.com/ollama/ollama?tab=readme-ov-file#ollama) 
to set up and run a local Ollama instance.

## LLM

```python
from langchain_ollama.llms import OllamaLLM
```

See the notebook example [here](/docs/integrations/llms/ollama).

## Chat Models

### Chat Ollama

```python
from langchain_ollama.chat_models import ChatOllama
```

See the notebook example [here](/docs/integrations/chat/ollama).

### Ollama toolcalling
[Ollama toolcalling](https://ollama.com/blog/tool-support) uses the 
OpenAI compatible web server specification, and can be used with 
the default `BaseChatModel.bind_tools()` methods
as described [here](https://blog.langchain.dev/tool-calling-with-langchain/).
Make sure to select an ollama model that supports [tool calling](https://ollama.com/search?&c=tools).

## Embedding models

```python
from langchain_community.embeddings import OllamaEmbeddings
```

See the notebook example [here](/docs/integrations/text_embedding/ollama).


